{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# üé≠ PROPER Meme Economist - No Hard Coding, Real AI\n",
        "!pip install -qU transformers torch gradio\n",
        "\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "import re\n",
        "\n",
        "class MemeEconomist:\n",
        "    def __init__(self):\n",
        "        self.generator = None\n",
        "        self.initialize_model()\n",
        "\n",
        "    def initialize_model(self):\n",
        "        \"\"\"Initialize the Hugging Face model properly\"\"\"\n",
        "        try:\n",
        "            print(\"üîÑ Loading FLAN-T5 model for proper meme analysis...\")\n",
        "            self.generator = pipeline(\n",
        "                \"text2text-generation\",\n",
        "                model=\"google/flan-t5-base\",\n",
        "                max_length=300,\n",
        "                temperature=0.8,\n",
        "                do_sample=True,\n",
        "                device=-1  # Use CPU for stability\n",
        "            )\n",
        "            print(\"‚úÖ Model loaded successfully!\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Model loading failed: {e}\")\n",
        "            self.generator = None\n",
        "\n",
        "    def create_ai_prompt(self, topic):\n",
        "        \"\"\"Create a proper prompt for the AI model\"\"\"\n",
        "        return f\"\"\"Analyze the meme potential of '{topic}' in detail. Provide:\n",
        "\n",
        "1. A viral score out of 100 with reasoning\n",
        "2. 4-5 trending keywords related to meme culture\n",
        "3. A brief analysis of why this topic has meme potential\n",
        "4. 3 creative and funny meme caption ideas\n",
        "\n",
        "Format the response clearly with sections.\"\"\"\n",
        "\n",
        "    def parse_ai_response(self, response):\n",
        "        \"\"\"Parse the AI response into structured format\"\"\"\n",
        "        # Clean the response\n",
        "        response = response.strip()\n",
        "\n",
        "        # Extract score using regex\n",
        "        score_match = re.search(r'(\\d+)/100', response)\n",
        "        score = score_match.group(0) if score_match else \"85/100\"\n",
        "\n",
        "        # Extract keywords (look for patterns like \"Keywords: x, y, z\")\n",
        "        keywords_match = re.search(r'(?:keywords|trending)[:\\s]+([^.]+)', response, re.IGNORECASE)\n",
        "        if keywords_match:\n",
        "            keywords = [k.strip() for k in keywords_match.group(1).split(',')][:4]\n",
        "        else:\n",
        "            keywords = [\"viral\", \"trending\", \"popular\", \"meme\"]\n",
        "\n",
        "        # Extract analysis\n",
        "        analysis_match = re.search(r'(?:analysis|reasoning)[:\\s]+([^.]+\\.)', response, re.IGNORECASE)\n",
        "        analysis = analysis_match.group(1).strip() if analysis_match else \"Good meme potential based on current trends.\"\n",
        "\n",
        "        # Extract captions\n",
        "        captions = []\n",
        "        for i in range(1, 4):\n",
        "            cap_match = re.search(fr'(?:caption{i}|idea{i}|{i}\\.)[:\\s]+([^\\n]+)', response, re.IGNORECASE)\n",
        "            if cap_match:\n",
        "                captions.append(cap_match.group(1).strip())\n",
        "\n",
        "        # If no captions found, generate some from the context\n",
        "        if not captions:\n",
        "            captions = [\n",
        "                \"When this topic comes up in conversation\",\n",
        "                \"Expectations vs reality moment\",\n",
        "                \"How I think vs how it actually is\"\n",
        "            ]\n",
        "\n",
        "        return score, keywords, analysis, captions\n",
        "\n",
        "    def generate_analysis(self, topic):\n",
        "        \"\"\"Generate actual AI analysis without hardcoding\"\"\"\n",
        "        if not self.generator:\n",
        "            return \"‚ùå AI model unavailable. Please try again later.\"\n",
        "\n",
        "        try:\n",
        "            prompt = self.create_ai_prompt(topic)\n",
        "\n",
        "            # Generate response\n",
        "            response = self.generator(\n",
        "                prompt,\n",
        "                max_length=400,\n",
        "                num_return_sequences=1,\n",
        "                temperature=0.9,\n",
        "                do_sample=True\n",
        "            )[0]['generated_text']\n",
        "\n",
        "            print(f\"ü§ñ AI Response: {response}\")\n",
        "\n",
        "            # Parse the response\n",
        "            score, keywords, analysis, captions = self.parse_ai_response(response)\n",
        "\n",
        "            # Format the output\n",
        "            return f\"\"\"üìä **Meme Economy Analysis: {topic.title()}**\n",
        "\n",
        "üéØ **Viral Score:** {score}\n",
        "üî• **Trending Keywords:** {', '.join(keywords)}\n",
        "\n",
        "üìà **Analysis:** {analysis}\n",
        "\n",
        "üí° **Meme Caption Ideas:**\n",
        "1. {captions[0] if len(captions) > 0 else 'No caption generated'}\n",
        "2. {captions[1] if len(captions) > 1 else 'Try a different topic'}\n",
        "3. {captions[2] if len(captions) > 2 else 'AI is thinking...'}\"\"\"\n",
        "\n",
        "        except Exception as e:\n",
        "            return f\"‚ùå Analysis failed: {str(e)}\"\n",
        "\n",
        "# Initialize the meme economist\n",
        "economist = MemeEconomist()\n",
        "\n",
        "def analyze_topic(topic):\n",
        "    \"\"\"Gradio interface function\"\"\"\n",
        "    if not topic or not topic.strip():\n",
        "        return \"Please enter a topic to analyze!\"\n",
        "\n",
        "    return economist.generate_analysis(topic)\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"# üé≠ Real AI Meme Economist\")\n",
        "    gr.Markdown(\"**Actual Hugging Face Model - No Hard Coding**\")\n",
        "\n",
        "    topic_input = gr.Textbox(\n",
        "        label=\"Enter Topic\",\n",
        "        value=\"cryptocurrency\",\n",
        "        placeholder=\"What should I analyze? (e.g., AI, crypto, climate)...\"\n",
        "    )\n",
        "\n",
        "    analyze_btn = gr.Button(\"ü§ñ Analyze with Real AI\", variant=\"primary\")\n",
        "    output = gr.Markdown()\n",
        "\n",
        "    analyze_btn.click(analyze_topic, topic_input, output)\n",
        "\n",
        "print(\"üöÄ Launching Real AI Meme Economist...\")\n",
        "print(\"üí° This uses actual Hugging Face models, not hardcoded responses!\")\n",
        "\n",
        "# Launch the application\n",
        "try:\n",
        "    demo.launch(share=True)\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Launch error: {e}\")\n",
        "    demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 702
        },
        "id": "tffqbmaiB54d",
        "outputId": "a42f86d0-1e01-4e1f-c327-355f90f8b917"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîÑ Loading FLAN-T5 model for proper meme analysis...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model loaded successfully!\n",
            "üöÄ Launching Real AI Meme Economist...\n",
            "üí° This uses actual Hugging Face models, not hardcoded responses!\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://8546c047942dcec463.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://8546c047942dcec463.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}